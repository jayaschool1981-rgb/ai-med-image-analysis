{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f023205d",
   "metadata": {},
   "source": [
    "# AI-Powered Medical Image Analysis â€” Training Notebook (EfficientNetB0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de53d34e",
   "metadata": {},
   "source": [
    "This notebook prepares data, trains the model, evaluates it, and exports artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e19e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, math, itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "IMAGE_SIZE = (224,224)\n",
    "BATCH_SIZE = 16\n",
    "DATA_ROOT = \"data/samples\"\n",
    "OUTPUT_DIR = \"models/v1\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17081dbd",
   "metadata": {},
   "source": [
    "## Build datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94970d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    os.path.join(DATA_ROOT, \"train\"),\n",
    "    labels=\"inferred\", label_mode=\"binary\",\n",
    "    image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, color_mode=\"rgb\", shuffle=True, seed=42\n",
    ")\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    os.path.join(DATA_ROOT, \"val\"),\n",
    "    labels=\"inferred\", label_mode=\"binary\",\n",
    "    image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, color_mode=\"rgb\", shuffle=False\n",
    ")\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    os.path.join(DATA_ROOT, \"test\"),\n",
    "    labels=\"inferred\", label_mode=\"binary\",\n",
    "    image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, color_mode=\"rgb\", shuffle=False\n",
    ")\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.map(lambda x,y: (tf.cast(x, tf.float32)/255.0, y)).prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.map(lambda x,y: (tf.cast(x, tf.float32)/255.0, y)).prefetch(AUTOTUNE)\n",
    "test_ds = test_ds.map(lambda x,y: (tf.cast(x, tf.float32)/255.0, y)).prefetch(AUTOTUNE)\n",
    "class_names = [ 'NORMAL', 'PNEUMONIA' ]\n",
    "print(class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4d0dc1",
   "metadata": {},
   "source": [
    "## Model & Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9fd3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation():\n",
    "    return tf.keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.03),\n",
    "        layers.RandomZoom(0.1),\n",
    "        layers.RandomContrast(0.15),\n",
    "    ])\n",
    "\n",
    "base = tf.keras.applications.efficientnet.EfficientNetB0(include_top=False, weights=\"imagenet\", input_shape=IMAGE_SIZE+(3,))\n",
    "base.trainable = False\n",
    "inputs = layers.Input(shape=IMAGE_SIZE+(3,))\n",
    "x = augmentation()(inputs)\n",
    "x = base(x, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = models.Model(inputs, outputs)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss='binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.AUC(name='auc'),\n",
    "                       tf.keras.metrics.BinaryAccuracy(name='acc'),\n",
    "                       tf.keras.metrics.Precision(name='precision'),\n",
    "                       tf.keras.metrics.Recall(name='recall')])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b038d23",
   "metadata": {},
   "source": [
    "## Class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6b4eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = []\n",
    "for _, y in train_ds.unbatch():\n",
    "    y_all.append(int(y.numpy()))\n",
    "import numpy as np\n",
    "y_all = np.array(y_all)\n",
    "classes = np.unique(y_all)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_all)\n",
    "class_weights = {int(c): float(w) for c, w in zip(classes, weights)}\n",
    "class_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b4eede",
   "metadata": {},
   "source": [
    "## Train (frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776e993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(os.path.join(OUTPUT_DIR, \"best.h5\"),\n",
    "                                       monitor=\"val_auc\", mode=\"max\", save_best_only=True),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=5, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-6),\n",
    "    tf.keras.callbacks.CSVLogger(os.path.join(OUTPUT_DIR, \"training_log.csv\")),\n",
    "]\n",
    "hist = model.fit(train_ds, validation_data=val_ds, epochs=5, class_weight=class_weights, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e12a50b",
   "metadata": {},
   "source": [
    "## Fine-tune last blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486a0804",
   "metadata": {},
   "outputs": [],
   "source": [
    "base.trainable = True\n",
    "for layer in base.layers[:-40]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.AUC(name='auc'),\n",
    "                       tf.keras.metrics.BinaryAccuracy(name='acc'),\n",
    "                       tf.keras.metrics.Precision(name='precision'),\n",
    "                       tf.keras.metrics.Recall(name='recall')])\n",
    "hist2 = model.fit(train_ds, validation_data=val_ds, epochs=10, class_weight=class_weights, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca944388",
   "metadata": {},
   "source": [
    "## Evaluation, Confusion Matrix, ROC/PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1141fd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_true = np.concatenate([y.numpy().ravel() for _, y in test_ds], axis=0)\n",
    "y_prob = model.predict(test_ds).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "roc = roc_auc_score(y_true, y_prob)\n",
    "print(\"ROC AUC:\", roc)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "pr_auc = auc(recall, precision)\n",
    "print(\"PR AUC:\", pr_auc)\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names, rotation=45)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\")\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfb7b60",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d627b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(OUTPUT_DIR, \"model.h5\"))\n",
    "model.save(os.path.join(OUTPUT_DIR, \"saved_model\"))\n",
    "print(\"Saved to\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcf1a3d",
   "metadata": {},
   "source": [
    "## Grad-CAM utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name=None):\n",
    "    # attempt to locate last conv layer automatically\n",
    "    if last_conv_layer_name is None:\n",
    "        last_conv_layer_name = None\n",
    "        for layer in reversed(model.layers):\n",
    "            if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "                last_conv_layer_name = layer.name\n",
    "                break\n",
    "        if last_conv_layer_name is None:\n",
    "            # try base model\n",
    "            for layer in reversed(model.layers):\n",
    "                if hasattr(layer, 'layers'):\n",
    "                    for l2 in reversed(layer.layers):\n",
    "                        if isinstance(l2, tf.keras.layers.Conv2D):\n",
    "                            last_conv_layer_name = l2.name\n",
    "                            break\n",
    "                    if last_conv_layer_name:\n",
    "                        break\n",
    "\n",
    "    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(last_conv_layer_name).output, model.output])\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array)\n",
    "        loss = predictions[:, 0]\n",
    "    grads = tape.gradient(loss, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs), axis=-1)\n",
    "    heatmap = tf.maximum(heatmap, 0) / (tf.math.reduce_max(heatmap) + 1e-6)\n",
    "    return heatmap.numpy()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
